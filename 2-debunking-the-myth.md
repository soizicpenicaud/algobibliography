# 2.Debunking the myth of neutral, objective, efficient algorithms

### Castro, D. (2016). Data detractors are wrong: The rise of algorithms is a cause for hope and optimism. Center for Data Innovation. Available at: www.datainnovation.org/2016/10/data-detractors-are-wrong-the-rise-of-algorithms-is-a-cause-for-hope-and-optimism/

### Christin, A. (2016). From Daguerreotypes to Algorithms: Machines, Expertise, and Three Forms of Objectivity. SIGCAS Comput. Soc., 46(1), 27–32. https://doi.org/10.1145/2908216.2908220

**Review**: This article takes a historical perspective on the debate about algorithmic objectivity. Drawing on the work of historians of science Daston and Galison on scientific production since the eighteenth century, it argues that their distinction between the three forms of objectivity associated with technologies in the last two centuries can help make sense of the current debate about algorithms. “Truth-to-nature”, a vision of objectivity prevalent until the 1980s, has been supplanted by two visions: “mechanical objectivity”, defended by proponents of big data, and “trained judgment” (which considers objectivity as the result of interactions between experts and algorithms), corresponding to the current scholarly reaction.
Angèle Christin is an assistant professor in the Department of Communication and affiliated faculty in the Sociology Department and Program in Science, Technology, and Society at Stanford University. Her work on algorithms often calls for a more grounded analysis of algorithms in practice, either in the context of their use (like in her work “Algorithms in practice: Comparing web journalism and criminal justice” (2017), cited below) or, like here, in a longer historical context.
This article is crucial in understanding what qualities we give algorithms. Through its long-term perspective, it helps recontextualize current debates about algorithms, and define the terms of dispute. It reminds us that the questions they raise are neither new nor clear cut: “objectivity” is not one monolithic concept, but rather a “complex process involving both social and technological agents”. In this context, it makes little sense to claim that “algorithms are objective” (or saying the opposite).

### Dressel, J., & Farid, H. (2018). The accuracy, fairness, and limits of predicting recidivism | Science Advances. Science Advances, 4(1). Retrieved from http://advances.sciencemag.org/content/4/1/eaao5580.full

**Review**: This article presents the results of a computer science on the predictive justice algorithm COMPAS, which claims to predict the risk of recidivism. In the study, the researchers compare the actual recidivism rate of convicted individuals with 1.the recidivism risk score they had been given by COMPAS and 2.the recidivism risk score they were given by 20 people with no or limited criminal justice expertise who responded to an online survey on Amazon Mechanical Turk (the “classifiers”). The classifiers were given limited information on defendants, such as their age, gender and type of criminal offense. The results show that COMPAS and the classifiers reached the same accuracy rate of about 65%. They also show that although COMPAS claims to use 137 features to make a prediction, the same predictive accuracy can be achieved with only two features (age and total number of previous convictions). 
This work directly addresses the “efficiency claim” commonly made about algorithms and demonstrates that algorithms might not be as efficient and “magical”. It striking results are a good example to use to interrogate the efficiency and relevance of predictive algorithms. 

### Lessig, L. (2006). Code is Law. In Code Version 2.0 (pp. 1–8). New York: Basic Books.

**Review**: In this work, Lessig reminds us that cyberspace, albeit unconstrained by governmental regulations, is not immune to restrictions on its freedom. What regulate cyberspace and condition its values are the technical choices concerning the architecture and infrastructure of the Internet, which can more or less restrict our liberties such as privacy or freedom of information. Lessig finishes with a reminder of our collective responsibility to interrogate the choices made by those who code this infrastructure, and decide how we want to regulate cyberspace.
“Code is Law” is the first chapter of Lessig’s well-known book, Code and Other Laws of Cyberspace, first published in 1999. It is a key work about the regulation of cyberspace. Lessig is a former Professor of Law at Harvard Law School, and one of the founders of cyberlaw. This chapter is short and contains no citations; its claims are substantiated by the subsequent chapters of the book. “Code is Law”’s catchy title phrase is often used in public discourse to illustrate the fact that technology is neither neutral nor objective. One could criticize his argument for supporting a form of technological determinism that obfuscates the fact that technology not only shapes society, but is also shaped by it. This is one of the reasons why this short article is not usually used in science and technology studies. However, Lessig does admit that the code is itself shaped by the choices of coders.
Although the article does not focus on algorithms, it remains an important read in this context because of its historical importance and its political undertones. Rather than simply stating a reality (technologies are sociotechnical systems that shape our society as much as society shapes them), it calls for a true collective commitment to making sure the technology we develop guarantees our liberties.   

### Morozov, E. (2014, July 19). Why the internet of things could destroy the welfare state. The Guardian.  

### O’Reilly, T. (2013). Open Data and Algorithmic Regulation. In Beyond Transparency (pp. 289–300). Code for America Press.

### Winner, L. (1980). Do artifacts have politics? Daedalus, 121–136.
